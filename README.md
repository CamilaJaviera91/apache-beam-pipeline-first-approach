# üöÄ Pipelines (First Approach)

## ‚û∞ What's a pipeline?

‚úÖ Sequence of **data** processing **steps**, where the **output** of one step becomes the **input** for the next. <br>
‚úÖ Commonly used in **data engineering**, **machine learning**, and **software development** to automate **workflows** and ensure efficient processing.

## üÖ±Ô∏è What's Apache-Beam?

‚úÖ **Apache Beam** is a framework for **batch** and **streaming data processing**. <br>
‚úÖ It provides a **unified API** that can run on multiple execution engines. <br>
‚úÖ Works well with **Google Cloud Dataflow**, **Apache Flink**, and **Apache Spark**. <br>
‚úÖ Supports **Python**, **Java**, and **Go** for pipeline development.

### Install Apache Beam in Python

- To install Apache Beam in Python, follow these steps:

1Ô∏è‚É£ Install Using pip:

```
pip install apache-beam
```

2Ô∏è‚É£ _Install Apache Beam with Google Cloud Support (Optional)_: <br>
    - _This includes additional dependencies for Google Cloud Storage, Pub/Sub, and BigQuery_

```
pip install apache-beam[gcp]
```

3Ô∏è‚É£ Verify Installation:

```
import apache_beam as beam

print(beam.__version__)
```

## üìÑ What's a DataFrame

‚úÖ A **DataFrame** is a tabular structure for handling structured data in **Python**.<br>
‚úÖ It is part of the **pandas library** and supports fast **data manipulation**.<br>
‚úÖ It is widely used in **data science**, **analytics**, and **machine learning**.

### Install pandas

1Ô∏è‚É£ Install Using pip:

```
pip install pandas
```

2Ô∏è‚É£ Verify Installation: <br>

```
import pandas as pd

print(pd.__version__)
```

### Install scikit-learn and call a Dataset

1Ô∏è‚É£ Install Using pip: <br>

```
pip install scikit-learn
```

2Ô∏è‚É£ Verify Installation: <br>

```
import sklearn

print(sklearn.__version__)
```

3Ô∏è‚É£ Call a dataset to work with it: <br>

```
from sklearn.datasets import load_linnerud
```

4Ô∏è‚É£ Transform it into a DataFrame: <br>

```
from sklearn.datasets import load_linnerud

dt = dataset()
df = pd.DataFrame(dt.data, columns=dt.feature_names)

print(df)
```

---

## üíª pipeline.py

### üë®‚Äçüíª Explanation of the Code:

1Ô∏è‚É£ **Import Libraries:** <br>

- **apache_beam:** Used for building and running data processing pipelines.
- **load_linnerud:** A dataset from scikit-learn containing physiological and exercise data.    
- **pandas:** A powerful data manipulation library for handling structured data.

2Ô∏è‚É£ **Load and Prepare Data:** <br>

- The Linnerud dataset is loaded and converted into a Pandas DataFrame for easier manipulation.

3Ô∏è‚É£ **Define the Pipeline:** <br>

- A function run_pipeline is defined to encapsulate the pipeline logic.
- Within this function, a pipeline is created using the with statement, ensuring proper resource management.

4Ô∏è‚É£ **Create PCollection:** <br>

- The 'Chins' column from the DataFrame is converted to a list and then to a PCollection using beam.Create.

5Ô∏è‚É£ **Apply Transformation:** <br>

- A transformation is applied to the **PCollection** to check if each value is greater than 10. This is done using **beam.Map** with a **lambda** function.

6Ô∏è‚É£ **Output Results:** <br>

The results are printed to the console using **beam.Map(print)**.

7Ô∏è‚É£ **Execute the Pipeline:** <br>

- The **run_pipeline** function is called within the ***if __name__ == '__main__':*** block to ensure the pipeline runs when the script is executed directly.